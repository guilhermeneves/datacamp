{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Hadoop: HDFS and MapReduce\n",
    "\n",
    "HDFS is a distributed filesystem, similar to your computer, the difference it resides in other computers. Nowadays S3 replaces the use of it.\n",
    "\n",
    "![HDFS](HDFS.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## MapReduce\n",
    "\n",
    "It works similar to examples using dask, it splits the process as jobs as you can see in the image below, the main flaw in MapReduce is that too hard to write this Jobs and Hive tries to fix it. and also it uses a lot of disk writes one thing that Spark Solves.\n",
    "\n",
    "![MapReduce](./MapReduce.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Hive\n",
    "\n",
    "Hive Runs on Hadoop with its own language structured query: Hive SQL. In the background it creates a Map Reduce Job.\n",
    "\n",
    "![Hive](./hive.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Spark\n",
    "Spark solves the issue with too many disk writes from MapReduce, by giving priority to memory use. Spark Architecture reiles on RDD (Resilient Distributed Datasets ) similar to dataframes without named columns, you can think as a list of tuples.\n",
    "Spark enables two type of results: Transformation returns a transformed RDD (map or filter) and Action returns one value (count or first)\n",
    "\n",
    "### PySpark\n",
    "Python Interface to Spark, dataframe abstraction you can do operations very similar to pandas, PySpark and Spark take cara of all parallel computing omplexity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}